{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a24368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import time\n",
    "import hazm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d363aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '..\\\\dataset'\n",
    "STOP_WORDS_PATH = '..\\\\utils\\\\stopwords.txt'\n",
    "\n",
    "TAG_REGEX = re.compile(r'<.*?>.*?<.*?>')\n",
    "HEAD_REGEX = re.compile(r'&lt;head&gt;.*?&lt;/head&gt;')\n",
    "BODY_REGEX = re.compile(r'&lt;body.*?&gt;.*?&lt;/body&gt;')\n",
    "\n",
    "TEXT_REGEX = re.compile(r'gt;.+?lt;')\n",
    "PERSIAN_TEXT_REGEX = re.compile(r'[\\u0600-\\u06FF\\s]+')\n",
    "\n",
    "ALPHA = 0.3\n",
    "\n",
    "vocab_map = {}\n",
    "doc_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cafeb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(base_path=DATASET_PATH):\n",
    "    dirs = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            dirs.append(root + '\\\\' + file)\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87353a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords(path=STOP_WORDS_PATH):\n",
    "    words = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words.append(line.strip())\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cf366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pure_pages(file_path):\n",
    "    document = ''\n",
    "    \n",
    "    appending = False\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line == '</DOC>':\n",
    "                appending = False\n",
    "                \n",
    "                yield document\n",
    "                document = ''\n",
    "                \n",
    "            if appending:\n",
    "                document += line\n",
    "            \n",
    "            if line == '<DOC>':\n",
    "                appending = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4609c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(page):\n",
    "    doc, url, html = TAG_REGEX.findall(page)\n",
    "    \n",
    "    return {\n",
    "        'doc': int(doc[7:-8]),\n",
    "        'url': url[5:-6],\n",
    "        'html': html.lower()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4133c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(page):\n",
    "    items = {'head': [], 'body': []}\n",
    "    for head, body in zip(HEAD_REGEX.findall(page['html']), BODY_REGEX.findall(page['html'])):\n",
    "        items['head'].append(head)\n",
    "        items['body'].append(body)\n",
    "            \n",
    "    return {\n",
    "        'doc': page['doc'],\n",
    "        'url': page['url'],\n",
    "        'tags': items\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a64ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = hazm.Normalizer()\n",
    "stemmer = hazm.Stemmer()\n",
    "stopwords = read_stopwords()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    \n",
    "    text = normalizer.normalize(text)\n",
    "    words = hazm.word_tokenize(text)\n",
    "    words = map(lambda word: stemmer.stem(word), words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in stopwords or len(word) == 0:\n",
    "            continue\n",
    "            \n",
    "        tokens.append(word)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42dad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0\n",
    "def clean_text_matrix(texts):\n",
    "    global N\n",
    "    data = {'head': {}, 'body': {}}\n",
    "    counter = 0\n",
    "    \n",
    "    for document in texts:\n",
    "        counter += 1\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            print('1000 more docs processed')\n",
    "        \n",
    "        doc, url, tags = document['doc'], document['url'], document['tags']\n",
    "        \n",
    "        head = tags['head']\n",
    "        body = tags['body']\n",
    "        \n",
    "        if not head:\n",
    "            data['head'][doc] = Counter()\n",
    "        if not body:\n",
    "            data['body'][doc] = Counter()\n",
    "        \n",
    "        for item in head:\n",
    "            cache = []\n",
    "            for text in PERSIAN_TEXT_REGEX.findall(item):\n",
    "                text = text[4:-4].replace('nbsp', '\\u200c').replace('amp', '&').replace('&', '').strip().lower()\n",
    "                if len(text) > 5:\n",
    "                    cache.extend(preprocess(text))\n",
    "            data['head'][doc] = Counter(cache)\n",
    "            \n",
    "        for item in body:\n",
    "            cache = []\n",
    "            for text in PERSIAN_TEXT_REGEX.findall(item):\n",
    "                text = text[4:-4].replace('nbsp', '\\u200c').replace('amp', '&').replace('&', '').strip().lower()\n",
    "                if len(text) > 5:\n",
    "                    cache.extend(preprocess(text))\n",
    "            data['body'][doc] = Counter(cache)\n",
    "               \n",
    "    N = counter\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9adc426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_pages(files):\n",
    "    pages = []\n",
    "    \n",
    "    counter = 0\n",
    "    for file in files:\n",
    "        for page in read_pure_pages(file):\n",
    "            counter += 1\n",
    "            page_dict = parse_page(page)\n",
    "            text = parse_html(page_dict)\n",
    "            pages.append(text)\n",
    "\n",
    "            if counter % 1000 == 0:\n",
    "                print('1000 more pages has been read')\n",
    "                \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32326c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_df(data, keys):\n",
    "    df = {}\n",
    "    \n",
    "    for doc_key in keys:\n",
    "        for key, count in data['head'][doc_key].items():\n",
    "            df[key] = df.get(key, 0) + count\n",
    "        for key, count in data['body'][doc_key].items():\n",
    "            df[key] = df.get(key, 0) + count\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "421cff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_idf(data, doc_ids, main_key, helper_key, DF):\n",
    "    tf_idf = {}\n",
    "    \n",
    "    for doc_id in doc_ids:\n",
    "        word_count = sum(data[main_key][doc_id].values()) + sum(data[helper_key][doc_id].values())\n",
    "        \n",
    "        for token in data[main_key][doc_id].keys():\n",
    "            counter = data[main_key][doc_id][token] + data[helper_key][doc_id][token]\n",
    "            \n",
    "            tf = counter / word_count\n",
    "            df = DF[token]\n",
    "            idf = np.log((N + 1) / (df + 1))\n",
    "            \n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "            \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c56319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tf_idfs(head_if_idf, body_if_idf):\n",
    "    tf_idf = {}\n",
    "    \n",
    "    for key in head_if_idf:\n",
    "        tf_idf[key] = head_if_idf[key]\n",
    "        \n",
    "    for key in body_if_idf:\n",
    "        tf_idf[key] = body_if_idf[key] * ALPHA\n",
    "    \n",
    "    return tf_idf, len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84525fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index():\n",
    "    tic = time.time()\n",
    "    \n",
    "    files = read_docs()\n",
    "    pages = read_all_pages(files)\n",
    "    cleaned_data = clean_text_matrix(pages)\n",
    "    \n",
    "    doc_ids = [page['doc'] for page in pages]\n",
    "    df = calculate_df(cleaned_data, doc_ids)\n",
    "    tokens = list(df.keys())\n",
    "    N = len(df.keys())\n",
    "    \n",
    "    head_if_idf = build_tf_idf(cleaned_data, doc_ids, 'head', 'body', df)\n",
    "    body_if_idf = build_tf_idf(cleaned_data, doc_ids, 'body', 'head', df)\n",
    "    \n",
    "    tf_idf, _ = merge_tf_idfs(head_if_idf, body_if_idf)\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(f'total time: {(toc - tic) / 60} min(s)')\n",
    "        \n",
    "    return tf_idf, tokens, df, doc_ids, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43b8ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, query, tf_idf):\n",
    "    tokens = preprocess(query)\n",
    "    tokens = set(tokens)\n",
    "    query_weights = {}\n",
    "    for key in tf_idf:\n",
    "        if key[1] in tokens:\n",
    "            query_weights[key[0]] = query_weights.get(key[0], 0) + tf_idf[key]\n",
    "            \n",
    "    query_weights = sorted(query_weights.items(), key = lambda x: x[1], reverse=True)\n",
    "    res = []\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "         res.append(i[0])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57ad07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_page(head, body):\n",
    "    print('**************************************************')\n",
    "    for item in head:\n",
    "        for text in PERSIAN_TEXT_REGEX.findall(item):\n",
    "            if len(text) > 5:\n",
    "                print(text)\n",
    "    print('---------------------------------------------------')\n",
    "    for item in body:\n",
    "        for text in PERSIAN_TEXT_REGEX.findall(item):\n",
    "            if len(text) > 5:\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20784209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, vocab, DF, doc_ids, pages = build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396289a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'كاربرد كامپيوتر در زيست شناسي ملك'\n",
    "indices = matching_score(2, query, tf_idf)\n",
    "# print(indices)\n",
    "for page_id in indices:\n",
    "    for page in pages:\n",
    "        if page['doc'] == page_id:\n",
    "            display_page(page['tags']['head'], page['tags']['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d6350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
